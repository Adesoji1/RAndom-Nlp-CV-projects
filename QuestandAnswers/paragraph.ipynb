{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 13:11:13.710079: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 65798144 exceeds 10% of free system memory.\n",
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import tensorflow\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from flask import Flask, request, jsonify\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config\n",
    "\n",
    "# Download CBT\n",
    "cbt_url = \"https://s3.amazonaws.com/text-datasets/CBTest/data/cbtest_CN_train.txt\"\n",
    "cbt_data = requests.get(cbt_url).text.splitlines()\n",
    "\n",
    "# Download SQuAD\n",
    "squad_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\"\n",
    "squad_data = json.loads(requests.get(squad_url).text)[\"data\"]\n",
    "\n",
    "# Process CBT\n",
    "def process_cbt_data(cbt_data):\n",
    "    passages = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "\n",
    "    passage = []\n",
    "    for line in cbt_data:\n",
    "        if line.startswith(\"_BOOK_TITLE_\"):\n",
    "            passage = []\n",
    "        elif line.startswith(\"21 \"):\n",
    "            parts = line[3:].split(\"\\t\")\n",
    "            questions.append(parts[0])\n",
    "            answers.append(parts[2])\n",
    "            passages.append(\" \".join(passage))\n",
    "        else:\n",
    "            passage.append(line)\n",
    "            if len(passage) > 20:\n",
    "                passage.pop(0)\n",
    "\n",
    "    return pd.DataFrame({\"passage\": passages, \"question\": questions, \"answer\": answers})\n",
    "\n",
    "\n",
    "cbt_df = process_cbt_data(cbt_data)\n",
    "\n",
    "# Process SQuAD\n",
    "def process_squad_data(squad_data):\n",
    "    passages = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "\n",
    "    for article in squad_data:\n",
    "        for paragraph in article[\"paragraphs\"]:\n",
    "            passage = paragraph[\"context\"]\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                if not qa[\"is_impossible\"]:\n",
    "                    questions.append(qa[\"question\"])\n",
    "                    answers.append(qa[\"answers\"][0][\"text\"])\n",
    "                    passages.append(passage)\n",
    "\n",
    "    return pd.DataFrame({\"passage\": passages, \"question\": questions, \"answer\": answers})\n",
    "\n",
    "squad_df = process_squad_data(squad_data)\n",
    "\n",
    "# Combine and split the datasets\n",
    "combined_df = pd.concat([cbt_df, squad_df], ignore_index=True)\n",
    "train_df, test_df = train_test_split(combined_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the preprocessed data\n",
    "train_df.to_csv(\"train_data.csv\", index=False)\n",
    "test_df.to_csv(\"test_data.csv\", index=False)\n",
    "\n",
    "# Phase 3: Model Selection\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "# Download the T5 config\n",
    "config = T5Config.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Load the TensorFlow model\n",
    "tf_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\", from_tf=True, config=config)\n",
    "\n",
    "# Save the PyTorch model\n",
    "tf_model.save_pretrained(\"t5-small-pytorch\")\n",
    "\n",
    "# Load the PyTorch model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small-pytorch\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\",model_max_length=512)\n",
    "\n",
    "# Phase 4: Model Training\n",
    "def tokenize_data(df):\n",
    "    inputs = tokenizer(df[\"question\"].tolist(), return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    targets = tokenizer(df[\"answer\"].tolist(), return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    return inputs, targets\n",
    "\n",
    "train_inputs, train_targets = tokenize_data(train_df)\n",
    "train_data = DataLoader(list(zip(train_inputs[\"input_ids\"], train_targets[\"input_ids\"])), batch_size=8, shuffle=True)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = batch\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        outputs = model(input_ids=inputs, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "# #This Runs on the cpu\n",
    "# model.train()\n",
    "# for epoch in range(3):\n",
    "#     for batch in train_data:\n",
    "#         optimizer.zero_grad()\n",
    "#         inputs, targets = batch\n",
    "#         outputs = model(input_ids=inputs, labels=targets)\n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# Phase 5: Model Evaluation\n",
    "test_inputs, test_targets = tokenize_data(test_df)\n",
    "test_data = DataLoader(list(zip(test_inputs[\"input_ids\"], test_targets[\"input_ids\"])), batch_size=8)\n",
    "\n",
    "#on Gpu\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "for batch in test_data:\n",
    "    inputs, targets = batch\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=inputs, labels=targets)\n",
    "    total_loss += outputs.loss.item()\n",
    "\n",
    "average_test_loss = total_loss / len(test_data)\n",
    "\n",
    "#on cpu\n",
    "# model.eval()\n",
    "# total_loss = 0\n",
    "# for batch in test_data:\n",
    "#     inputs, targets = batch\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(input_ids=inputs, labels=targets)\n",
    "#     total_loss += outputs.loss.item()\n",
    "\n",
    "# average_test_loss = total_loss / len(test_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "[2023-04-08 14:22:32,232] ERROR in app: Exception on /generate_questions [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/flask/app.py\", line 2528, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/flask/app.py\", line 1825, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/flask/app.py\", line 1823, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/flask/app.py\", line 1799, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "  File \"/tmp/ipykernel_739/2190046079.py\", line 14, in generate_questions\n",
      "    output_tokens = model.generate(**tokenized_input)\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/transformers/generation/utils.py\", line 1268, in generate\n",
      "    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/transformers/generation/utils.py\", line 634, in _prepare_encoder_decoder_kwargs_for_generation\n",
      "    model_kwargs[\"encoder_outputs\"]: ModelOutput = encoder(**encoder_kwargs)\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py\", line 973, in forward\n",
      "    inputs_embeds = self.embed_tokens(input_ids)\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/torch/nn/modules/sparse.py\", line 162, in forward\n",
      "    return F.embedding(\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/torch/nn/functional.py\", line 2210, in embedding\n",
      "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
      "127.0.0.1 - - [08/Apr/2023 14:22:32] \"POST /generate_questions HTTP/1.1\" 500 -\n",
      "127.0.0.1 - - [08/Apr/2023 14:22:50] \"GET / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return \"Hello, World!\"\n",
    "\n",
    "@app.route('/generate_questions', methods=['POST'])\n",
    "def generate_questions():\n",
    "    input_text = request.json['input_text']\n",
    "    tokenized_input = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(**tokenized_input)\n",
    "    generated_questions = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n",
    "    return jsonify(generated_questions)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "#cpu Alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [08/Apr/2023 14:25:48] \"GET / HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [08/Apr/2023 14:25:51] \"GET / HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [08/Apr/2023 14:25:53] \"GET / HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [08/Apr/2023 14:25:54] \"GET / HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [08/Apr/2023 14:26:38] \"GET /generate_questions HTTP/1.1\" 405 -\n",
      "/home/adesoji/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [08/Apr/2023 14:27:57] \"POST /generate_questions HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Apr/2023 14:28:05] \"POST /generate_questions HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Apr/2023 14:28:28] \"POST /generate_questions HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Apr/2023 14:30:04] \"POST /generate_questions HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/generate_questions', methods=['POST'])\n",
    "def generate_questions():\n",
    "    input_text = request.json['input_text']\n",
    "    tokenized_input = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Move input tensors to GPU\n",
    "    tokenized_input = {key: tensor.to(\"cuda\") for key, tensor in tokenized_input.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(**tokenized_input)\n",
    "    generated_questions = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n",
    "    return jsonify(generated_questions)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -X POST -H \"Content-Type: application/json\" -d '{\"input_text\": \"Children playing in a park on a sunny day\"}' http://127.0.0.1:5000/generate_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import tensorflow\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from flask import Flask, request, jsonify\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config\n",
    "\n",
    "# # Load the local datasets\n",
    "# cbt_df = pd.read_csv(\"CBTest_CN_train.csv\")\n",
    "# squad_df = pd.read_csv(\"SQuAD_train-v2.0.csv\")\n",
    "\n",
    "# # Assuming the CSV files have columns: 'passage', 'question', 'answer'\n",
    "# # If not, you may need to rename the columns or adjust the following code accordingly\n",
    "\n",
    "# # Combine and split the datasets\n",
    "# combined_df = pd.concat([cbt_df, squad_df], ignore_index=True)\n",
    "# train_df, test_df = train_test_split(combined_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Save the preprocessed data\n",
    "# train_df.to_csv(\"train_data.csv\", index=False)\n",
    "# test_df.to_csv(\"test_data.csv\", index=False)\n",
    "train_df =pd.read_csv('/home/adesoji/targetdir/test_data.csv')\n",
    "test_df = pd.read_csv('/home/adesoji/targetdir/test_data.csv')\n",
    "\n",
    "\n",
    "# Phase 3: Model Selection\n",
    "# model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "# tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "# Download the T5 config\n",
    "config = T5Config.from_pretrained(\"t5-small\")\n",
    "\n",
    "# Load the TensorFlow model\n",
    "tf_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\", from_tf=True, config=config)\n",
    "\n",
    "# Save the PyTorch model\n",
    "tf_model.save_pretrained(\"t5-small-pytorch\")\n",
    "\n",
    "# Load the PyTorch model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small-pytorch\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\",model_max_length=512)\n",
    "\n",
    "# Phase 4: Model Training\n",
    "def tokenize_data(df):\n",
    "    question_prefix = \"generate a question: \"\n",
    "    inputs = tokenizer([question_prefix + question for question in df[\"question\"].tolist()], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    targets = tokenizer(df[\"answer\"].tolist(), return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    return inputs, targets\n",
    "\n",
    "\n",
    "train_inputs, train_targets = tokenize_data(train_df)\n",
    "train_data = DataLoader(list(zip(train_inputs[\"input_ids\"], train_targets[\"input_ids\"])), batch_size=8, shuffle=True)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "\n",
    "model.train()\n",
    "for epoch in range(3):\n",
    "    for batch in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = batch\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        outputs = model(input_ids=inputs, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "# #This Runs on the cpu\n",
    "# model.train()\n",
    "# for epoch in range(3):\n",
    "#     for batch in train_data:\n",
    "#         optimizer.zero_grad()\n",
    "#         inputs, targets = batch\n",
    "#         outputs = model(input_ids=inputs, labels=targets)\n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "# Phase 5: Model Evaluation\n",
    "test_inputs, test_targets = tokenize_data(test_df)\n",
    "test_data = DataLoader(list(zip(test_inputs[\"input_ids\"], test_targets[\"input_ids\"])), batch_size=8)\n",
    "\n",
    "#on Gpu\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "for batch in test_data:\n",
    "    inputs, targets = batch\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=inputs, labels=targets)\n",
    "    total_loss += outputs.loss.item()\n",
    "\n",
    "average_test_loss = total_loss / len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "/home/adesoji/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "127.0.0.1 - - [08/Apr/2023 15:05:20] \"POST /generate_questions HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [08/Apr/2023 15:05:33] \"POST /generate_questions HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/generate_questions', methods=['POST'])\n",
    "def generate_questions():\n",
    "    input_text = request.json['input_text']\n",
    "    tokenized_input = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Move input tensors to GPU\n",
    "    tokenized_input = {key: tensor.to(\"cuda\") for key, tensor in tokenized_input.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(**tokenized_input)\n",
    "    generated_questions = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)\n",
    "    return jsonify(generated_questions)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/adesoji/nltk_data/RAndom-Nlp-CV-projects/Q&A/test_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mflask\u001b[39;00m \u001b[39mimport\u001b[39;00m Flask, request, jsonify\n\u001b[1;32m      9\u001b[0m \u001b[39m# Load the local datasets\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m train_df \u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39m/home/adesoji/nltk_data/RAndom-Nlp-CV-projects/Q&A/test_data.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     11\u001b[0m test_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m/home/adesoji/nltk_data/RAndom-Nlp-CV-projects/Q&A/test_data.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[39m# Model Selection and model loading\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1736\u001b[0m     f,\n\u001b[1;32m   1737\u001b[0m     mode,\n\u001b[1;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1744\u001b[0m )\n\u001b[1;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/adesoji/nltk_data/RAndom-Nlp-CV-projects/Q&A/test_data.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "# Load the local datasets\n",
    "train_df =pd.read_csv('QuestandAnswers/test_data.csv')\n",
    "test_df = pd.read_csv('QuestandAnswers/test_data.csv')\n",
    "\n",
    "# Model Selection and model loading\n",
    "config = T5Config.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small-pytorch\", config=config)\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", model_max_length=512)\n",
    "\n",
    "# Model Training\n",
    "def tokenize_data(df):\n",
    "    inputs = tokenizer(df[\"question\"].tolist(), return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    targets = tokenizer(df[\"answer\"].tolist(), return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    return inputs, targets\n",
    "\n",
    "train_inputs, train_targets = tokenize_data(train_df)\n",
    "train_data = DataLoader(list(zip(train_inputs[\"input_ids\"], train_targets[\"input_ids\"])), batch_size=8, shuffle=True)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(12):\n",
    "    total_train_loss = 0\n",
    "    for batch in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = batch\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "        outputs = model(input_ids=inputs, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    average_train_loss = total_train_loss / len(train_data)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {average_train_loss:.4f}\")\n",
    "\n",
    "\n",
    "# Model Evaluation\n",
    "test_inputs, test_targets = tokenize_data(test_df)\n",
    "test_data = DataLoader(list(zip(test_inputs[\"input_ids\"], test_targets[\"input_ids\"])), batch_size=8)\n",
    "\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "for batch in test_data:\n",
    "    inputs, targets = batch\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=inputs, labels=targets)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        correct += (predictions == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "average_test_loss = total_loss / len(test_data)\n",
    "accuracy = correct / total\n",
    "print(f\"Validation Loss: {average_test_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Model Deployment\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/generate_answer', methods=['POST'])\n",
    "def generate_answer():\n",
    "    input_data = request.json\n",
    "    input_text = input_data.get('passage', '') + ' ' + input_data.get('question', '')\n",
    "    tokenized_input = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    if torch.cuda.is_available():\n",
    "        tokenized_input = {k: v.cuda() for k, v in tokenized_input.items()}\n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(**tokenized_input)\n",
    "    generated_answer = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[0]\n",
    "    return jsonify({\"answer\": generated_answer})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -X POST -H \"Content-Type: application/json\" -d '{\"passage\": \"Children were playing in a park on a sunny day.\", \"question\": \"Who is biden ?\"}' http://127.0.0.1:5000/generate_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "[2023-04-08 18:42:31,181] ERROR in app: Exception on /generate_answer [POST]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/flask/app.py\", line 2528, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/flask/app.py\", line 1825, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/flask/app.py\", line 1823, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/flask/app.py\", line 1799, in dispatch_request\n",
      "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
      "  File \"/tmp/ipykernel_36114/4266315263.py\", line 25, in generate_answer\n",
      "    output_tokens = model.generate(**tokenized_input)\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/transformers/generation/utils.py\", line 1268, in generate\n",
      "    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/transformers/generation/utils.py\", line 634, in _prepare_encoder_decoder_kwargs_for_generation\n",
      "    model_kwargs[\"encoder_outputs\"]: ModelOutput = encoder(**encoder_kwargs)\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py\", line 973, in forward\n",
      "    inputs_embeds = self.embed_tokens(input_ids)\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/torch/nn/modules/sparse.py\", line 162, in forward\n",
      "    return F.embedding(\n",
      "  File \"/home/adesoji/.local/lib/python3.8/site-packages/torch/nn/functional.py\", line 2210, in embedding\n",
      "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument index in method wrapper_CUDA__index_select)\n",
      "127.0.0.1 - - [08/Apr/2023 18:42:31] \"POST /generate_answer HTTP/1.1\" 500 -\n"
     ]
    }
   ],
   "source": [
    "# # Model Deployment on cpu\n",
    "# app = Flask(__name__)\n",
    "\n",
    "# @app.route('/generate_answer', methods=['POST'])\n",
    "# def generate_answer():\n",
    "#     input_data = request.json\n",
    "#     input_text = input_data.get('passage', '') + ' ' + input_data.get('question', '')\n",
    "#     tokenized_input = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "#     with torch.no_grad():\n",
    "#         output_tokens = model.generate(**tokenized_input)\n",
    "#     generated_answer = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[0]\n",
    "#     return jsonify({\"answer\": generated_answer})\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     app.run()\n",
    "#Model Deployment\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/generate_answer', methods=['POST'])\n",
    "def generate_answer():\n",
    "    input_data = request.json\n",
    "    input_text = input_data.get('passage', '') + ' ' + input_data.get('question', '')\n",
    "    tokenized_input = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        output_tokens = model.generate(**tokenized_input)\n",
    "    generated_answer = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[0]\n",
    "    return jsonify({\"answer\": generated_answer})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
